{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK DATA CLEANING\n",
    "\n",
    "- Remember to set Python kernel to 3 (not later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: File operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Copy file's contents to another file; this works for text files...\n",
    "\n",
    "with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/README.txt','r') as sourcefile:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/DESTFILE.txt','w') as destfile:\n",
    "        for line in sourcefile:\n",
    "            destfile.write(line)\n",
    "\n",
    "\n",
    "### And for an image file...simply append `b` for *binary mode* to file operation command `r`, `w`, or `a`\n",
    "\n",
    "with open('/Users/vix/OneDrive/Temp/Portrait_Vikram_Before-After.png','rb') as sourceimage:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/destimage.png','wb') as destimage:\n",
    "        for line in sourceimage:\n",
    "            destimage.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Get book text from Project Gutenberg, save to file, and populate list object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import os, requests, re, urllib\n",
    "\n",
    "### os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Texts')\n",
    "gutenberg_texts = [] # Initialize list\n",
    "\n",
    "for counter in range(45001,50001): # Loop over each book, which is a reference number\n",
    "\n",
    "    def get_gutenberg_text():\n",
    "        url = \"https://www.gutenberg.org/files/\" + str(counter) + \"/\" + str(counter) + \".txt\"\n",
    "\n",
    "        try: # Check if URL valid\n",
    "            webpage = urllib.request.urlopen(url) # Open the webpage containing book text\n",
    "\n",
    "            # Extract book title and author (author TBD) for file name\n",
    "            linecount = 1\n",
    "            for line in webpage:\n",
    "                m = re.search('Title: ',str(line))\n",
    "                if m:\n",
    "                    print(\"Matched!\")\n",
    "                    text = line.decode()\n",
    "                    booktitle = text[7 : (len(text) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                linecount += 1 # Advance line counter\n",
    "            filename = str(counter)+'_'+booktitle+'.txt'\n",
    "            \n",
    "            # Write book text to output file\n",
    "            print(\"Currently retrieving: \" + booktitle + \" -- file name: \" + filename)\n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Corpora/Gutenberg Texts/' + filename),'w') as file:\n",
    "                webpage = urllib.request.urlopen(url)\n",
    "                for line in webpage:\n",
    "                    text = line.decode() # IMP: Extract only text, discarding non-printing characters\n",
    "                    file.write(text)\n",
    "            \n",
    "            # Write book text to list \n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Corpora/Gutenberg Texts/' + filename),'r') as file:\n",
    "                text = [file.read().replace('\\n','')]\n",
    "                gutenberg_texts.append(text)\n",
    "                print(\"Added list item: \" + str(len(gutenberg_texts)) + \"\\n\") # Enumerate list count, which is number of books\n",
    "            return gutenberg_texts\n",
    "\n",
    "\n",
    "        except: # If URL invalid, means no book at that webpage\n",
    "            print(\"URL Not Valid\\n\")\n",
    "\n",
    "    gutenberg_texts = get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Separate function to populate list object using existing files in given folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract book title, author, & text\n",
    "\n",
    "import os, codecs, re\n",
    "\n",
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books') # Set working folder\n",
    "print(\"Adding *all* files in \" + os.getcwd() + \"\\n\")\n",
    "gutenberg_titles = [] # Initialize list of titles\n",
    "gutenberg_authors = [] # Initialize list of authors\n",
    "gutenberg_texts = [] # Initialize list of texts\n",
    "\n",
    "def get_gutenberg_text():\n",
    "    for file in sorted(listdir_nohidden(\".\")):\n",
    "        with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "\n",
    "            # First, take title from file name\n",
    "            title = re.sub(\"\\.txt\",\"\",file)\n",
    "            title = re.sub(\"\\./\",\"\",title)\n",
    "            title = re.sub(\"^\\d+_\",\"\",title)\n",
    "            gutenberg_titles.append(title) # Put book title in list\n",
    "\n",
    "            # Then, extract author from book text\n",
    "            author = \"\"\n",
    "            for line in f: \n",
    "                if re.search('Author: ',str(line)):\n",
    "                    author = line[8 : (len(line) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                    gutenberg_authors.append(author) # Put author in list\n",
    "                    print(\"Found author: \" + author)\n",
    "                    break\n",
    "                elif re.search(\"^BY \",str(line)):\n",
    "                    author = line[3 : (len(line) -2)]\n",
    "                    gutenberg_authors.append(author) # Put author in list\n",
    "                    print(\"Found author: \" + author)\n",
    "                    break\n",
    "            if not author:\n",
    "                print(\"No Author Match for \" + title)\n",
    "                gutenberg_authors.append(\"Unknown Author\") # If no author found\n",
    "                \n",
    "            # Finally, read all text, beginning with official \"START OF\" line\n",
    "            gutenberg_text_current = []\n",
    "            f.seek(0)\n",
    "            started = False\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                if not started:\n",
    "                    if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "                        started = True\n",
    "                        print (\"Started at line\", i+1) \n",
    "                elif re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "                    print (\"Ended at line\", i+1)\n",
    "                    break\n",
    "                else:\n",
    "                    gutenberg_text_current.append(line)\n",
    "            \n",
    "            print(len(gutenberg_text_current))\n",
    "            if len(gutenberg_text_current) == 0:\n",
    "                f.seek(0)\n",
    "                text = f.read()\n",
    "                gutenberg_text_current.append(text)\n",
    "                gutenberg_texts.append(gutenberg_text_current)\n",
    "            else:\n",
    "                gutenberg_text_current[0 : len(gutenberg_text_current)] = [''.join(gutenberg_text_current[0 : len(gutenberg_text_current)])] \n",
    "                gutenberg_texts.append(gutenberg_text_current)\n",
    "\n",
    "            print(\"Added list index # \" + str(len(gutenberg_texts)-1) + \": \" + title + \"\\n\") # Enumerate running count of books\n",
    "\n",
    "get_gutenberg_text() # Call function\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Correct code snippet used to read only specified block of text beginning and ending with some condition\n",
    "\n",
    "with open(\"/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books/11_Alice's Adventures in Wonderland.txt\", 'r') as f:\n",
    "\n",
    "    started = False\n",
    "    collected_lines = []\n",
    "    combined_lines = []\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if not started:\n",
    "            if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "                started = True\n",
    "                print (\"started at line\", i+1) \n",
    "        elif re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "            print (\"end at line\", i+1)\n",
    "            break\n",
    "        else:\n",
    "            collected_lines.append(line)\n",
    "    print(\"Book done\")\n",
    "    combined_lines[0 : len(collected_lines)] = [''.join(collected_lines[0 : len(collected_lines)])] \n",
    "\n",
    "### Prior wrong attempts at above\n",
    "\n",
    "with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "\n",
    "            f.seek(0)\n",
    "            started = False\n",
    "            collected_lines = []\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                    if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "                        started = True\n",
    "                        print (\"started at line\", i) \n",
    "                        continue\n",
    "                    if started and re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "                        print (\"end at line\", i)\n",
    "                        break\n",
    "                    collected_lines.append(line)\n",
    "\n",
    "            f.seek(0)\n",
    "            textstart = 0\n",
    "            for line in f:\n",
    "                if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "                    textstart = textstart + len(line) + 4 # Find start position\n",
    "                    text = f.readline()\n",
    "                    if re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "                        textend = textstart + len(line) + 4 # Find end position\n",
    "                        break\n",
    "                else:\n",
    "                    textstart = textstart + len(line)\n",
    "                    f.seek(textstart)\n",
    "            \n",
    "            text = f.read() # Begin reading at current value of `textstart`\n",
    "            \n",
    "            if not text: # In case no starting point found, take it all\n",
    "                f.seek(0)\n",
    "                text = f.read()\n",
    "\n",
    "            text = text.replace('\\n',' ')\n",
    "            text = text.replace('\\r','')\n",
    "            \n",
    "            gutenberg_texts.append(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract book author\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books') # Set working folder\n",
    "print(\"Adding *all* files in \" + os.getcwd() + \"\\n\")\n",
    "gutenberg_authors = [] # Initialize list of book titles\n",
    "\n",
    "for file in sorted(listdir_nohidden(\".\")):\n",
    "    with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        author = \"\"\n",
    "        linecount = 1\n",
    "        for line in f:\n",
    "            if re.search('Author: ',str(line)):\n",
    "                print(\"Matched!\")\n",
    "                author = line[8 : (len(line) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                gutenberg_authors.append(author) # Put authors in list\n",
    "            linecount += 1 # Advance line counter\n",
    "        if not author:\n",
    "            print(\"No Match for \" + file)\n",
    "            gutenberg_authors.append(\"Unknown Author\") # If no author found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Useful code to display beginnings of each list item as preview\n",
    "[book[0][:100] for book in gutenberg_texts]\n",
    "\n",
    "### Similar code for items in dictionary form\n",
    "{}\n",
    "\n",
    "### Useful code to convert MS Word document to text file\n",
    "\n",
    "import docx2txt\n",
    "converted_text = docx2txt.process(filename.docx)\n",
    "with open(Filename.txt, 'w') as file:\n",
    "    file.write(converted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle files for later use - alternative to `csv_writer()`?\n",
    "\n",
    "### Make a new directory to hold the text files\n",
    "!mkdir transcripts\n",
    "\n",
    "for i, c in enumerate(comedians):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(transcripts[i], file) ### Indexing into the `transcripts` array/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load pickled files into dictionary data container\n",
    "#### - ** IMP ** Modified to create dictionary directly from respective list items\n",
    "\n",
    "books_dict = {} #### `{}` signifies a dictionary\n",
    "books_dict = dict(zip(gutenberg_titles, gutenberg_texts)) #### `dict()` constructor w/ `zip` iterator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful dictionary syntax\n",
    "\n",
    "### Dict comprehension creates dictionaries from arbitrary key:value expressions\n",
    "#### - `test_dict = {x: \"This is the text for book \" + x for x in gutenberg_titles}`\n",
    "#### - `test_dict = {key: key*2}\n",
    "#### - Comprehension also available for object types list and set\n",
    "### There are three \"dictionary views\"\n",
    "**** - `dict.keys()`, `dict.values()`, `dict_items()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Double check to make sure data has been loaded properly\n",
    "#### `books.keys()` This is the original syntax from the lesson; however, output is a block of running keys.  \n",
    "\n",
    "### I find the following syntax better, resulting in a more elegant list of keys\n",
    "list(books_dict)\n",
    "### or `sorted(books)`\n",
    "### `'key' in dict` checks for presence of given key in dictionary (or `not in`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More checks - need to learn how to display only first few lines of dictionary item\n",
    "books['Q and the Magic of Grammar'][:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that our dictionary is currently in key: book title, value: list of text format\n",
    "#### - for some reason our values are not in list of text form - they are already string\n",
    "### next(iter(books.values()))\n",
    "books.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - We are going to change this to key: book title, value: string format - NOT USED\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "### Combine it!\n",
    "books_combined = {key: [combine_text(value)] for (key, value) in books.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either keep it in dictionary format or put it into a **PANDAS DATAFRAME**\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "books_df = pd.DataFrame.from_dict(books_dict,orient='index')\n",
    "books_df.columns = ['book_text']\n",
    "# books_df = books_df.sort_index()\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add author to dataframe as column\n",
    "books_df['author_name'] = gutenberg_authors\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Explore components of dataframe\n",
    "books_df.columns\n",
    "# books_df.loc['Q and the Magic of Grammar']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text_round1):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text_round1 = text_round1.lower()\n",
    "    text_round1 = re.sub('\\[.*?\\]', '', text_round1)\n",
    "    text_round1 = re.sub('\\*', '', text_round1)\n",
    "    text_round1 = re.sub('\\,', '', text_round1)\n",
    "    text_round1 = re.sub('\\;', '', text_round1)\n",
    "    text_round1 = re.sub('\\:', '', text_round1)\n",
    "    text_round1 = re.sub('\\.', '', text_round1)\n",
    "    text_round1 = re.sub('\\\"', '', text_round1)\n",
    "    text_round1 = re.sub('\\w*\\d\\w*', '', text_round1)\n",
    "    return text_round1\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)# Apply the first round of text cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text from round 1\n",
    "books_df_clean = pd.DataFrame(books_df.book_text.apply(round1))\n",
    "books_df_clean['author_name'] = gutenberg_authors\n",
    "books_df_clean['book_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text_round2):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text_round2 = re.sub('[‘’“”…]', '', text_round2)\n",
    "    text_round2 = re.sub('_', '', text_round2)\n",
    "    text_round2 = re.sub('\\n', ' ', text_round2)\n",
    "    text_round2 = re.sub('\\r', ' ', text_round2)\n",
    "    return text_round2\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x) # Apply the second round of text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text from round 2\n",
    "books_df_clean = pd.DataFrame(books_df_clean.book_text.apply(round2))\n",
    "books_df_clean['author_name'] = gutenberg_authors\n",
    "books_df_clean['book_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a third round to deal with idiomatic and idiosyncratic deliberate mis-spellings\n",
    "\n",
    "def clean_text_round3(text_round3):\n",
    "    text_round3 = re.sub(\"in\\'\", 'ing', text_round3)\n",
    "    text_round3 = re.sub(\"^\\'em\", 'them', text_round3)\n",
    "    text_round3 = re.sub('^.?-', '', text_round3)\n",
    "    return text_round3\n",
    "\n",
    "round3 = lambda x: clean_text_round3(x) # Apply the third round of text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text from round 3\n",
    "books_df_clean = pd.DataFrame(books_df_clean.book_text.apply(round3))\n",
    "books_df_clean['author_name'] = gutenberg_authors\n",
    "books_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our dataframe\n",
    "books_df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use - original and cleaned\n",
    "books_df.to_pickle(\"/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/pickle/books_corpus.pkl\")\n",
    "books_df_clean.to_pickle(\"/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/pickle/books_clean_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "## First, make custom stopwords list from previously edited built-in list\n",
    "books_stopwords = pd.read_csv('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books_stopwords.csv', header=None, encoding='utf-8')\n",
    "books_stopwords = list(np.squeeze(books_stopwords.values))\n",
    "    # with open(\"/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books_stopwords.csv\", 'r') as csvfile:\n",
    "    #     books_stopwords = []\n",
    "    #     books_stopwords = list(csv.reader(csvfile))\n",
    "\n",
    "cv = CountVectorizer(stop_words=frozenset(books_stopwords), strip_accents='ascii', token_pattern=r'(?u)\\b\\w+\\b', ngram_range=(1,1))\n",
    "books_cv = cv.fit_transform(books_df_clean.book_text)\n",
    "# print(cv.get_feature_names())\n",
    "\n",
    "## Save resultant words - called feature names - to file\n",
    "with open(\"/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books_featurenames_vocabulary.csv\", 'w', newline='') as csvfile:\n",
    "    vocab = csv.writer(csvfile, delimiter='\\n')\n",
    "    vocab.writerow(cv.get_feature_names())\n",
    "\n",
    "books_dtm = pd.DataFrame(books_cv.toarray(), columns=cv.get_feature_names())\n",
    "books_dtm.index = books_df_clean.index\n",
    "books_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(books_dtm.loc[:, \"p\"]) ### Check if certain term exists in vocabulary\n",
    "except:\n",
    "    print(\"\\nThat text does not exist\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's pickle it for later use\n",
    "books_dtm.to_pickle(\"/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/pickle/books_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's also pickle the cleaned data in dataframe form (before we had put it in document-term matrix format), as well as the CountVectorizer object\n",
    "books_df_clean.to_pickle('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/pickle/books_df_clean.pkl')\n",
    "import pickle\n",
    "pickle.dump(cv, open(\"/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/pickle/books_cv.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594575264080",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}