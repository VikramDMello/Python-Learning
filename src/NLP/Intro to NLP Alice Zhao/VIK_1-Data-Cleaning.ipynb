{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK DATA CLEANING\n",
    "\n",
    "- Remember to set Python kernel to 3 (not later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "import os, requests, pandas as pd, pickle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: File operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Essentially copying a file's contents to another file - this works for text files...\n",
    "\n",
    "with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/README.txt','r') as sourcefile:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/DESTFILE.txt','w') as destfile:\n",
    "        for line in sourcefile:\n",
    "            destfile.write(line)\n",
    "\n",
    "\n",
    "### And for an image file...simply append `b` for *binary mode* to file operation command `r`, `w`, or `a`\n",
    "\n",
    "with open('/Users/vix/OneDrive/Temp/Portrait_Vikram_Before-After.png','rb') as sourceimage:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/destimage.png','wb') as destimage:\n",
    "        for line in sourceimage:\n",
    "            destimage.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Get book text from Project Gutenberg, save to file, and populate list object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "import re\n",
    "import urllib # Import `urllib` package - primarily using `request` module with `urlopen` method\n",
    "\n",
    "### os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Texts')\n",
    "gutenberg_texts = [] # Initialize list\n",
    "\n",
    "for counter in range(10,25003): # Loop over each book, which is a reference number\n",
    "\n",
    "    def get_gutenberg_text():\n",
    "        url = \"https://www.gutenberg.org/files/\" + str(counter) + \"/\" + str(counter) + \".txt\"\n",
    "\n",
    "        try: # Check if URL valid\n",
    "            webpage = urllib.request.urlopen(url) # Open the webpage containing book text\n",
    "\n",
    "            # Extract book title and author (author TBD) for file name\n",
    "            linecount = 1\n",
    "            for line in webpage:\n",
    "                m = re.search('Title: ',str(line))\n",
    "                if m:\n",
    "                    print(\"Matched!\")\n",
    "                    text = line.decode()\n",
    "                    booktitle = text[7 : (len(text) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                linecount += 1 # Advance line counter\n",
    "            filename = str(counter)+'_'+booktitle+'.txt'\n",
    "            \n",
    "            # Write book text to output file\n",
    "            print(\"Currently retrieving: \" + booktitle + \" -- file name: \" + filename)\n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Texts/' + filename),'w') as file:\n",
    "                webpage = urllib.request.urlopen(url)\n",
    "                for line in webpage:\n",
    "                    text = line.decode() # IMP: Extract only text, discarding non-printing characters\n",
    "                    file.write(text)\n",
    "            \n",
    "            # Write book text to list \n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Texts/' + filename),'r') as file:\n",
    "                text = [file.read().replace('\\n','')]\n",
    "                gutenberg_texts.append(text)\n",
    "                print(\"Added list item: \" + str(len(gutenberg_texts)) + \"\\n\") # Enumerate list count, which is number of books\n",
    "            return gutenberg_texts\n",
    "\n",
    "\n",
    "        except: # If URL invalid, means no book at that webpage\n",
    "            print(\"URL Not Valid\\n\")\n",
    "\n",
    "    gutenberg_texts = get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Separate function to populate list object using existing files in given folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Extract book title, author, & text\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books') # Set working folder\n",
    "print(\"Adding *all* files in \" + os.getcwd() + \"\\n\")\n",
    "gutenberg_titles = [] # Initialize list of titles\n",
    "gutenberg_authors = [] # Initialize list of authors\n",
    "gutenberg_texts = [] # Initialize list of texts\n",
    "\n",
    "def get_gutenberg_text():\n",
    "    for file in sorted(listdir_nohidden(\".\")):\n",
    "        with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "\n",
    "            # First, take title from file name\n",
    "            title = re.sub(\"\\.txt\",\"\",file)\n",
    "            title = re.sub(\"\\./\",\"\",title)\n",
    "            title = re.sub(\"^\\d+_\",\"\",title)\n",
    "            gutenberg_titles.append(title) # Put book title in list\n",
    "\n",
    "            # Then, extract author from book text\n",
    "            author = \"\"\n",
    "            for line in f: \n",
    "                if re.search('Author: ',str(line)):\n",
    "                    author = line[8 : (len(line) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                    gutenberg_authors.append(author) # Put author in list\n",
    "                    print(\"Found author: \" + author)\n",
    "                    break\n",
    "                elif re.search(\"^BY \",str(line)):\n",
    "                    author = line[3 : (len(line) -2)]\n",
    "                    gutenberg_authors.append(author) # Put author in list\n",
    "                    print(\"Found author: \" + author)\n",
    "                    break\n",
    "            if not author:\n",
    "                print(\"No Author Match for \" + title)\n",
    "                gutenberg_authors.append(\"Unknown Author\") # If no author found\n",
    "\n",
    "            # Finally, read all text, beginning with official \"START OF\" line\n",
    "\n",
    "    with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "\n",
    "            f.seek(0)\n",
    "            started = False\n",
    "            collected_lines = []\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                    if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "                        started = True\n",
    "                        print (\"started at line\", i) \n",
    "                        continue\n",
    "                    if started and re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "                        print (\"end at line\", i)\n",
    "                        break\n",
    "                    collected_lines.append(line)\n",
    "\n",
    "            # f.seek(0)\n",
    "            # textstart = 0\n",
    "            # for line in f:\n",
    "            #     if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "            #         textstart = textstart + len(line) + 4 # Find start position\n",
    "            #         text = f.readline()\n",
    "            #         if re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "            #             textend = textstart + len(line) + 4 # Find end position\n",
    "            #             break\n",
    "            #     else:\n",
    "            #         textstart = textstart + len(line)\n",
    "            #         f.seek(textstart)\n",
    "            \n",
    "            # text = f.read() # Begin reading at current value of `textstart`\n",
    "            \n",
    "            # if not text: # In case no starting point found, take it all\n",
    "            #     f.seek(0)\n",
    "            #     text = f.read()\n",
    "\n",
    "            # text = text.replace('\\n',' ')\n",
    "            # text = text.replace('\\r','')\n",
    "            \n",
    "            # gutenberg_texts.append(text)\n",
    "            \n",
    "            print(\"Added \" + str(len(gutenberg_texts)) + \": \" + title + \"\\n\") # Enumerate running count of books\n",
    "\n",
    "get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-46-17c1faca654e>, line 18)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-17c1faca654e>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    print pd.groupby('value')[collected_lines].apply(' '.join).reset_index()\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os, re, pandas as pd\n",
    "\n",
    "with open(\"/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books/11_Alice's Adventures in Wonderland.txt\", 'r') as f:\n",
    "\n",
    "    started = False\n",
    "    collected_lines = []\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if not started:\n",
    "            if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "                started = True\n",
    "                print (\"started at line\", i+1) \n",
    "        elif re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "            print (\"end at line\", i+1)\n",
    "            break\n",
    "        else:\n",
    "            collected_lines.append(line)\n",
    "    print(\"Book done\")\n",
    "    print df.groupby('value')[collected_lines].apply(' '.join).reset_index()\n",
    "    value \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract book author\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books') # Set working folder\n",
    "print(\"Adding *all* files in \" + os.getcwd() + \"\\n\")\n",
    "gutenberg_authors = [] # Initialize list of book titles\n",
    "\n",
    "for file in sorted(listdir_nohidden(\".\")):\n",
    "    with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        author = \"\"\n",
    "        linecount = 1\n",
    "        for line in f:\n",
    "            if re.search('Author: ',str(line)):\n",
    "                print(\"Matched!\")\n",
    "                author = line[8 : (len(line) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                gutenberg_authors.append(author) # Put authors in list\n",
    "            linecount += 1 # Advance line counter\n",
    "        if not author:\n",
    "            print(\"No Match for \" + file)\n",
    "            gutenberg_authors.append(\"Unknown Author\") # If no author found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Useful code to display beginnings of each list item as preview\n",
    "[book[0][:100] for book in gutenberg_texts]\n",
    "\n",
    "### Similar code for items in dictionary form\n",
    "{}\n",
    "\n",
    "### Useful code to convert MS Word document to text file\n",
    "\n",
    "import docx2txt\n",
    "converted_text = docx2txt.process(filename.docx)\n",
    "with open(Filename.txt, 'w') as file:\n",
    "    file.write(converted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle files for later use - alternative to `csv_writer()`?\n",
    "\n",
    "### Make a new directory to hold the text files\n",
    "!mkdir transcripts\n",
    "\n",
    "for i, c in enumerate(comedians):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(transcripts[i], file) ### Indexing into the `transcripts` array/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load pickled files into dictionary data container\n",
    "#### - Modified code to simply open file; unsure about need for Pickle\n",
    "\n",
    "### Create dictionary data container which can hold book text as well as title, from respective list objects\n",
    "books = {} # `{}` signifies a dictionary\n",
    "for i, title in enumerate(gutenberg_titles):\n",
    "    with open(title + \".txt\", 'r') as f:\n",
    "        books[title] = f.read()\n",
    "# books = {'title': gutenberg_titles, 'text': gutenberg_texts}\n",
    "# print(len(books))\n",
    "# for key, value in books.items():\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check to make sure data has been loaded properly\n",
    "books.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More checks\n",
    "books['Q and the Magic of Grammar'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's take a look at our data again\n",
    "### next(iter(books.keys()))\n",
    "books.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice that our dictionary is currently in key: book title, value: list of text format\n",
    "#### - for some reason our values are not in list of text form - they are already string\n",
    "### next(iter(books.values()))\n",
    "books.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - We are going to change this to key: book title, value: string format - NOT USED\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "### Combine it!\n",
    "books_combined = {key: [combine_text(value)] for (key, value) in books.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "books_df = pd.DataFrame.from_dict(books,orient='index')\n",
    "books_df.columns = ['book_text']\n",
    "books_df = books_df.sort_index()\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the text for title Q and the Magic of Grammar\n",
    "books_df.book_text.loc['Q and the Magic of Grammar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text_round1):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text_round1 = text_round1.lower()\n",
    "    text_round1 = re.sub('\\[.*?\\]', '', text_round1)\n",
    "    text_round1 = re.sub('[%s]' % re.escape(string.punctuation), '', text_round1)\n",
    "    text_round1 = re.sub('\\w*\\d\\w*', '', text_round1)\n",
    "    return text_round1\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)# Apply a first round of text cleaning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text from round 1\n",
    "books_clean = pd.DataFrame(books_df.book_text.apply(round1))\n",
    "books_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text_round2):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text_round2 = re.sub('[‘’“”…]', '', text_round2)\n",
    "    text_round2 = re.sub('\\n', ' ', text_round2)\n",
    "    return text_round2\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "books_clean = pd.DataFrame(books_clean.book_text.apply(round2))\n",
    "books_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our dataframe\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's add the author's name as well\n",
    "\n",
    "books_df['book_author'] = gutenberg_authors\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "books_df.to_pickle(\"books_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "books_cv = cv.fit_transform(books_clean.book_text)\n",
    "books_dtm = pd.DataFrame(books_cv.toarray(), columns=cv.get_feature_names())\n",
    "books_dtm.index = books_clean.index\n",
    "books_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's pickle it for later use\n",
    "books_dtm.to_pickle(\"books_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "books_clean.to_pickle('books_clean.pkl')\n",
    "pickle.dump(cv, open(\"books_cv.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}