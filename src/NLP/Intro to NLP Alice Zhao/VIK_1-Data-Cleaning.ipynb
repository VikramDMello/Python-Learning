{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK DATA CLEANING\n",
    "\n",
    "- Remember to set Python kernel to 3 (not later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "import os, requests, pandas as pd, pickle\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: File operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Essentially copying a file's contents to another file - this works for text files...\n",
    "\n",
    "with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/README.txt','r') as sourcefile:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/DESTFILE.txt','w') as destfile:\n",
    "        for line in sourcefile:\n",
    "            destfile.write(line)\n",
    "\n",
    "\n",
    "### And for an image file...simply append `b` for *binary mode* to file operation command `r`, `w`, or `a`\n",
    "\n",
    "with open('/Users/vix/OneDrive/Temp/Portrait_Vikram_Before-After.png','rb') as sourceimage:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/destimage.png','wb') as destimage:\n",
    "        for line in sourceimage:\n",
    "            destimage.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Get book text from Project Gutenberg, save to file, and populate list object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "import re\n",
    "import urllib # Import `urllib` package - primarily using `request` module with `urlopen` method\n",
    "\n",
    "### os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Texts')\n",
    "gutenberg_texts = [] # Initialize list\n",
    "\n",
    "for counter in range(10,25003): # Loop over each book, which is a reference number\n",
    "\n",
    "    def get_gutenberg_text():\n",
    "        url = \"https://www.gutenberg.org/files/\" + str(counter) + \"/\" + str(counter) + \".txt\"\n",
    "\n",
    "        try: # Check if URL valid\n",
    "            webpage = urllib.request.urlopen(url) # Open the webpage containing book text\n",
    "\n",
    "            # Extract book title and author (author TBD) for file name\n",
    "            linecount = 1\n",
    "            for line in webpage:\n",
    "                m = re.search('Title: ',str(line))\n",
    "                if m:\n",
    "                    print(\"Matched!\")\n",
    "                    text = line.decode()\n",
    "                    booktitle = text[7 : (len(text) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                linecount += 1 # Advance line counter\n",
    "            filename = str(counter)+'_'+booktitle+'.txt'\n",
    "            \n",
    "            # Write book text to output file\n",
    "            print(\"Currently retrieving: \" + booktitle + \" -- file name: \" + filename)\n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Texts/' + filename),'w') as file:\n",
    "                webpage = urllib.request.urlopen(url)\n",
    "                for line in webpage:\n",
    "                    text = line.decode() # IMP: Extract only text, discarding non-printing characters\n",
    "                    file.write(text)\n",
    "            \n",
    "            # Write book text to list \n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Texts/' + filename),'r') as file:\n",
    "                text = [file.read().replace('\\n','')]\n",
    "                gutenberg_texts.append(text)\n",
    "                print(\"Added list item: \" + str(len(gutenberg_texts)) + \"\\n\") # Enumerate list count, which is number of books\n",
    "            return gutenberg_texts\n",
    "\n",
    "\n",
    "        except: # If URL invalid, means no book at that webpage\n",
    "            print(\"URL Not Valid\\n\")\n",
    "\n",
    "    gutenberg_texts = get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Separate function to populate list object using existing files in given folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Adding *all* files in /Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books\n\nFound author: Dante Alighieri\nstarted at line 19\nend at line 5309\nAdded 0: Hell\n\nFound author: Thomas Troward\nstarted at line 24\nend at line 3365\nAdded 0: The Creative Process in the Individual\n\nFound author: Kossuth\nstarted at line 18\nend at line 15210\nAdded 0: Select Speeches of Kossuth\n\nFound author: Unknown\nstarted at line 19\nend at line 803\nAdded 0: The Apricot Tree\n\nNo Author Match for The King James Bible\nstarted at line 27\nend at line 99873\nAdded 0: The King James Bible\n\nNo Author Match for King Richard III\nAdded 0: King Richard III\n\nFound author: Various\nstarted at line 20\nend at line 1572\nAdded 0: The Mirror of Literature, Amusement, and Instruction\n\nFound author: Elizabeth Gray Potter and Mabel Thayer Gray\nstarted at line 20\nend at line 2455\nAdded 0: The Lure of San Francisco\n\nFound author: Thomas Alexander Browne, AKA Rolf Boldrewood\nstarted at line 20\nend at line 20267\nAdded 0: Robbery Under Arms\n\nFound author: Lewis Carroll\nstarted at line 20\nend at line 3377\nAdded 0: Alice's Adventures in Wonderland\n\nFound author: Beatrix Potter\nstarted at line 25\nend at line 397\nAdded 0: The Tale of Mrs. Tiggy-Winkle\n\nFound author: Edgar Rice Burroughs\nstarted at line 19\nend at line 5175\nAdded 0: At the Earth's Core\n\nFound author: Various\nstarted at line 19\nend at line 1914\nAdded 0: Punch, Or The London Charivari, Vol. 146., January 21, 1914\n\nFound author: Charles Dodgson, AKA Lewis Carroll\nstarted at line 21\nend at line 3947\nAdded 0: Through the Looking-Glass\n\nFound author: Jerome Klapka, AKA Jerome K. Jerome\nstarted at line 19\nend at line 15346\nAdded 0: Paul Kelver\n\nFound author: Lewis Carroll\nstarted at line 20\nend at line 881\nAdded 0: The Hunting of the Snark\n\nFound author: Aleister Crowley\nstarted at line 18\nend at line 1045\nAdded 0: Household Gods\n\nFound author: James M. Barrie\nstarted at line 23\nend at line 6252\nAdded 0: Peter Pan\n\nFound author: Honore de Balzac\nstarted at line 21\nend at line 597\nAdded 0: Facino Cane\n\nFound author: Anonymous\nstarted at line 24\nend at line 39921\nAdded 0: The Book Of Mormon\n\nFound author: Alexander Hamilton\nstarted at line 23\nend at line 25617\nAdded 0: The Federalist Papers\n\nFound author: Henry W. Longfellow\nstarted at line 20\nend at line 6457\nAdded 0: The Song Of Hiawatha\n\nFound author: John Milton\nstarted at line 19\nend at line 11046\nAdded 0: Paradise Lost\n\nFound author: P. G. Wodehouse\nstarted at line 21\nend at line 10342\nAdded 0: A Damsel in Distress\n\nNo Author Match for Sketches in Lavender, Blue and Green\nAdded 0: Sketches in Lavender, Blue and Green\n\nFound author: Gotthold Ephraim Lessing\nstarted at line 21\nend at line 5285\nAdded 0: Minna von Barnhelm\n\nFound author: Gene Stratton Porter\nstarted at line 18\nend at line 16242\nAdded 0: Laddie\n\nFound author: William Hazlitt\nstarted at line 21\nend at line 14403\nAdded 0: Table-Talk\n\nFound author: Dr. G.V. (C.V.) Legros\nAdded 0: Fabre, Poet of Science\n\nFound author: Madame du Hausset, and of an Unknown English Girl and the Princess Lamballe\nstarted at line 22\nend at line 3278\nAdded 0: The Memoirs of Louis XV. and XVI., Volume 6\n\nFound author: Unknown\nstarted at line 21\nend at line 592\nAdded 0: Se-Quo-Yah; from Harper's New Monthly, V. 41, 1870\n\nFound author: David Graham Phillips\nstarted at line 19\nend at line 3821\nAdded 0: The Fortune Hunter\n\nFound author: Orison Swett Marden\nstarted at line 21\nend at line 5459\nAdded 0: Eclectic School Readings: Stories from Life\n\nFound author: John Lothrop Motley\nstarted at line 43\nend at line 1621\nAdded 0: The Rise of the Dutch Republic, 1577-78\n\nFound author: Joseph C. Lincoln\nstarted at line 19\nend at line 6253\nAdded 0: Cape Cod Stories\n\nFound author: Georg Ebers\nstarted at line 43\nend at line 1458\nAdded 0: Arachne, Volume 1.\n\nFound author: Charlotte M. Yonge\nstarted at line 19\nend at line 24068\nAdded 0: The Young Step-Mother\n\nFound author: Harriet E. Wilson\nstarted at line 21\nend at line 2999\nAdded 0: Our nig\n\nFound author: Carolyn Wells\nstarted at line 20\nend at line 7849\nAdded 0: Vicky Van\n\nFound author: Francis W. Parker and Nellie Lathrop Helm\nstarted at line 39\nend at line 5252\nAdded 0: Uncle Robert's Geography (Uncle Robert's Visit, V.3)\n\nFound author: Isabella Valancy Crawford\nstarted at line 20\nend at line 7334\nAdded 0: Old Spookses' Pass\n\nFound author: Henry Ford\nstarted at line 38\nend at line 9012\nAdded 0: My Life and Work\n\nFound author: Edward Bulwer-Lytton\nstarted at line 43\nend at line 1704\nAdded 0: Ernest Maltravers, Book 9\n\nFound author: Charles Dickens\nstarted at line 20\nend at line 38230\nAdded 0: David Copperfield\n\nFound author: Anne Bronte\nstarted at line 26\nend at line 6864\nAdded 0: Agnes Grey\n\nNo Author Match for The Bible, King James version, Book 21: Ecclesiastes\nstarted at line 42\nend at line 847\nAdded 0: The Bible, King James version, Book 21: Ecclesiastes\n\nNo Author Match for The Bible, Douay-Rheims, Book 69: 1 John\nstarted at line 42\nend at line 625\nAdded 0: The Bible, Douay-Rheims, Book 69: 1 John\n\nFound author: William Wordsworth\nstarted at line 40\nend at line 4248\nAdded 0: Lyrical Ballads with Other Poems, 1800, Vol. 2\n\nFound author: Josiah Allen's Wife (Marietta Holley)\nstarted at line 19\nend at line 1252\nAdded 0: Samantha Among the Brethren, Part 4.\n\nFound author: Jessie Graham Flower\nstarted at line 20\nend at line 6025\nAdded 0: Grace Harlowe's Return to Overton Campus\n\nNo Author Match for Q and the Magic of Grammar\nAdded 0: Q and the Magic of Grammar\n\n"
    }
   ],
   "source": [
    "### Extract book title, author, & text\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books') # Set working folder\n",
    "print(\"Adding *all* files in \" + os.getcwd() + \"\\n\")\n",
    "gutenberg_titles = [] # Initialize list of titles\n",
    "gutenberg_authors = [] # Initialize list of authors\n",
    "gutenberg_texts = [] # Initialize list of texts\n",
    "\n",
    "def get_gutenberg_text():\n",
    "    for file in sorted(listdir_nohidden(\".\")):\n",
    "        with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "\n",
    "            # First, take title from file name\n",
    "            title = re.sub(\"\\.txt\",\"\",file)\n",
    "            title = re.sub(\"\\./\",\"\",title)\n",
    "            title = re.sub(\"^\\d+_\",\"\",title)\n",
    "            gutenberg_titles.append(title) # Put book title in list\n",
    "\n",
    "            # Then, extract author from book text\n",
    "            author = \"\"\n",
    "            for line in f: \n",
    "                if re.search('Author: ',str(line)):\n",
    "                    author = line[8 : (len(line) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                    gutenberg_authors.append(author) # Put author in list\n",
    "                    print(\"Found author: \" + author)\n",
    "                    break\n",
    "                elif re.search(\"^BY \",str(line)):\n",
    "                    author = line[3 : (len(line) -2)]\n",
    "                    gutenberg_authors.append(author) # Put author in list\n",
    "                    print(\"Found author: \" + author)\n",
    "                    break\n",
    "            if not author:\n",
    "                print(\"No Author Match for \" + title)\n",
    "                gutenberg_authors.append(\"Unknown Author\") # If no author found\n",
    "\n",
    "            # Finally, read all text, beginning with official \"START OF\" line\n",
    "\n",
    "            f.seek(0)\n",
    "            started = False\n",
    "            collected_lines = []\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                    if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "                        started = True\n",
    "                        print (\"started at line\", i) \n",
    "                        continue\n",
    "                    if started and re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "                        print (\"end at line\", i)\n",
    "                        break\n",
    "                    collected_lines.append(line)\n",
    "\n",
    "            # f.seek(0)\n",
    "            # textstart = 0\n",
    "            # for line in f:\n",
    "            #     if re.search('^\\*\\*\\*.?START OF',str(line)):\n",
    "            #         textstart = textstart + len(line) + 4 # Find start position\n",
    "            #         text = f.readline()\n",
    "            #         if re.search('^\\*\\*\\*.?END OF',str(line)):\n",
    "            #             textend = textstart + len(line) + 4 # Find end position\n",
    "            #             break\n",
    "            #     else:\n",
    "            #         textstart = textstart + len(line)\n",
    "            #         f.seek(textstart)\n",
    "            \n",
    "            # text = f.read() # Begin reading at current value of `textstart`\n",
    "            \n",
    "            # if not text: # In case no starting point found, take it all\n",
    "            #     f.seek(0)\n",
    "            #     text = f.read()\n",
    "\n",
    "            # text = text.replace('\\n',' ')\n",
    "            # text = text.replace('\\r','')\n",
    "            \n",
    "            # gutenberg_texts.append(text)\n",
    "            \n",
    "            print(\"Added \" + str(len(gutenberg_texts)) + \": \" + title + \"\\n\") # Enumerate running count of books\n",
    "\n",
    "get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract book author\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books') # Set working folder\n",
    "print(\"Adding *all* files in \" + os.getcwd() + \"\\n\")\n",
    "gutenberg_authors = [] # Initialize list of book titles\n",
    "\n",
    "for file in sorted(listdir_nohidden(\".\")):\n",
    "    with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        author = \"\"\n",
    "        linecount = 1\n",
    "        for line in f:\n",
    "            if re.search('Author: ',str(line)):\n",
    "                print(\"Matched!\")\n",
    "                author = line[8 : (len(line) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                gutenberg_authors.append(author) # Put authors in list\n",
    "            linecount += 1 # Advance line counter\n",
    "        if not author:\n",
    "            print(\"No Match for \" + file)\n",
    "            gutenberg_authors.append(\"Unknown Author\") # If no author found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Useful code to display beginnings of each list item as preview\n",
    "[book[0][:100] for book in gutenberg_texts]\n",
    "\n",
    "### Similar code for items in dictionary form\n",
    "{}\n",
    "\n",
    "### Useful code to convert MS Word document to text file\n",
    "\n",
    "import docx2txt\n",
    "converted_text = docx2txt.process(filename.docx)\n",
    "with open(Filename.txt, 'w') as file:\n",
    "    file.write(converted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle files for later use - alternative to `csv_writer()`?\n",
    "\n",
    "### Make a new directory to hold the text files\n",
    "!mkdir transcripts\n",
    "\n",
    "for i, c in enumerate(comedians):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(transcripts[i], file) ### Indexing into the `transcripts` array/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Load pickled files into dictionary data container\n",
    "#### - Modified code to simply open file; unsure about need for Pickle\n",
    "\n",
    "### Create dictionary data container which can hold book text as well as title, from respective list objects\n",
    "books = {} # `{}` signifies a dictionary\n",
    "for i, title in enumerate(gutenberg_titles):\n",
    "    with open(title + \".txt\", 'r') as f:\n",
    "        books[title] = f.read()\n",
    "# books = {'title': gutenberg_titles, 'text': gutenberg_texts}\n",
    "# print(len(books))\n",
    "# for key, value in books.items():\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check to make sure data has been loaded properly\n",
    "books.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More checks\n",
    "books['Q and the Magic of Grammar'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's take a look at our data again\n",
    "### next(iter(books.keys()))\n",
    "books.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notice that our dictionary is currently in key: book title, value: list of text format\n",
    "#### - for some reason our values are not in list of text form - they are already string\n",
    "### next(iter(books.values()))\n",
    "books.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - We are going to change this to key: book title, value: string format - NOT USED\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ''.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "### Combine it!\n",
    "books_combined = {key: [combine_text(value)] for (key, value) in books.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "books_df = pd.DataFrame.from_dict(books,orient='index')\n",
    "books_df.columns = ['book_text']\n",
    "books_df = books_df.sort_index()\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the text for title Q and the Magic of Grammar\n",
    "books_df.book_text.loc['Q and the Magic of Grammar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text_round1):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text_round1 = text_round1.lower()\n",
    "    text_round1 = re.sub('\\[.*?\\]', '', text_round1)\n",
    "    text_round1 = re.sub('[%s]' % re.escape(string.punctuation), '', text_round1)\n",
    "    text_round1 = re.sub('\\w*\\d\\w*', '', text_round1)\n",
    "    return text_round1\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)# Apply a first round of text cleaning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text from round 1\n",
    "books_clean = pd.DataFrame(books_df.book_text.apply(round1))\n",
    "books_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a second round of cleaning\n",
    "def clean_text_round2(text_round2):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text_round2 = re.sub('[‘’“”…]', '', text_round2)\n",
    "    text_round2 = re.sub('\\n', ' ', text_round2)\n",
    "    return text_round2\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "books_clean = pd.DataFrame(books_clean.book_text.apply(round2))\n",
    "books_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our dataframe\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's add the author's name as well\n",
    "\n",
    "books_df['book_author'] = gutenberg_authors\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "books_df.to_pickle(\"books_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "books_cv = cv.fit_transform(books_clean.book_text)\n",
    "books_dtm = pd.DataFrame(books_cv.toarray(), columns=cv.get_feature_names())\n",
    "books_dtm.index = books_clean.index\n",
    "books_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's pickle it for later use\n",
    "books_dtm.to_pickle(\"books_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
    "books_clean.to_pickle('books_clean.pkl')\n",
    "pickle.dump(cv, open(\"books_cv.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}