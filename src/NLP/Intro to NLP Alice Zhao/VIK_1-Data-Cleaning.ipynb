{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK DATA CLEANING\n",
    "\n",
    "- Remember to set Python kernel to 3 (not later).\n",
    "- Install additional packages `textblob`, `wordcloud`, and `gensim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Search Engine Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages for scraping webpage contents and making sense of them\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variables and lists to serve as argument placeholders for scraping\n",
    "\n",
    "- #### `get` works in conjunction with `requests`.\n",
    "- #### `BeautifulSoup` must have a particular HTML element from a webpage to work on.\n",
    "  + ##### In this case, `class=\"post-content\">`, and the *p* is from `<p style=...>`.\n",
    "  + ##### Each website might have its own HTML structure; so might need different `soup.find` argument for each site being scraped.  \n",
    "    * ##### E.g., `class_=\"css-53u6y8\"` works for a NYTimes.com article, along with  *p* which is standard in HTML to represent a paragraph of text.\n",
    "    * ##### E.g., `class_=\"repo-list\"` works for GitHub search results.\n",
    "    * ##### E.g., `li class_=\"b_algo\"` with *a* works for Bing search results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ask user for input\n",
    "\n",
    "os.system('clear')\n",
    "\n",
    "print(\"\\n\\nHello there!  \\n\\n\\nThis tool takes your search query, \\n\\napplies it to both major search engines, \\n\\nand then displays a simple comparison of the resulting search engine results.  \\n\\nAnalysis is limited to the first 100 results from each search engine.\")\n",
    "\n",
    "search_query = input(\"\\n\\n\\nWhat should we search for? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set page URLs\n",
    "query_url_bing = requests.get('https://www.bing.com/search?'+'q='+search_query+'&count=1000').text\n",
    "query_url_google = requests.get('https://www.google.com/search?'+'q='+search_query+'&num=1000').text\n",
    "\n",
    "#### Pass URL of page into `BeautifulSoup` method\n",
    "query_html_bing = BeautifulSoup(query_url_bing, 'lxml')\n",
    "query_html_google = BeautifulSoup(query_url_google, \"html.parser\")\n",
    "\n",
    "### Test page extraction results\n",
    "#### Display well-formatted HTML results\n",
    "##### print (source_html.prettify())\n",
    "\n",
    "#### Extract one result title\n",
    "##### result_title = source_html.find('div', class_='f4 text-normal').text\n",
    "#### Extract one result description\n",
    "##### result_desc = source_html.find('p', class_='mb-1').text\n",
    "\n",
    "#### Display one (ie, first) result and title\n",
    "##### print (result_title)\n",
    "##### print (result_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare output file\n",
    "\n",
    "import csv # CSV module\n",
    "from datetime import datetime\n",
    "\n",
    "#### Check existing records in file, to which we will append\n",
    "\n",
    "##### with open('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/Search_Results_Combined.csv','r') as csv_file:\n",
    "#####     csv_reader = csv.reader(csv_file)\n",
    "#####     for line in csv_reader:\n",
    "#####         print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract each search result's title and append to CSV file - note `'a'` argument for append\n",
    "\n",
    "#### Since using csv.writer in append mode, no need for header row; but otherwise for new files must use:\n",
    "##### csv_writer.writerow(['Search_Engine','Search_Query','Result_Title'])\n",
    "\n",
    "with open('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/Search_Results_Combined.csv','a') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    for result in query_html_bing.find_all('li', attrs = {'class':'b_algo'}):\n",
    "        result_title_bing = result.find('a').text\n",
    "        # print(result_title_bing) - Avoid to save display space\n",
    "        csv_writer.writerow([datetime.today(),\"Bing\",search_query,result_title_bing])\n",
    "\n",
    "    for result in query_html_google.find_all('h3', attrs = {'class':'zBAuLc'}):\n",
    "        result_title_google = result.find('div').text\n",
    "        # print(result_title_google) - Avoid to save display space\n",
    "        csv_writer.writerow([datetime.today(),\"Google\",search_query,result_title_google])\n",
    "\n",
    "csv_file.close() # Must close file, only after completing loop of row additions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Compare two text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import urllib\n",
    "import os\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Texts')\n",
    "\n",
    "with open('base.txt', 'r') as file1:\n",
    "    with open('comparison.txt', 'r') as file2:\n",
    "        difference = set(file1).symmetric_difference(file2)\n",
    "\n",
    "for line in difference:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Retrieve contents of text **file** at any URL and save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: File operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Essentially copying a file's contents to another file - this works for text files...\n",
    "\n",
    "with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/README.txt','r') as sourcefile:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/DESTFILE.txt','w') as destfile:\n",
    "        for line in sourcefile:\n",
    "            destfile.write(line)\n",
    "\n",
    "\n",
    "### And for an image file...simply append `b` for *binary mode* to file operation command `r`, `w`, or `a`\n",
    "\n",
    "with open('/Users/vix/OneDrive/Temp/Portrait_Vikram_Before-After.png','rb') as sourceimage:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/destimage.png','wb') as destimage:\n",
    "        for line in sourceimage:\n",
    "            destimage.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Get book text from Project Gutenberg, save to file, and populate list object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import urllib # Import `urllib` package - primarily using `request` module with `urlopen` method\n",
    "\n",
    "# os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Texts')\n",
    "gutenberg_texts = [] # Initialize list\n",
    "\n",
    "for counter in range(10,25003): # Loop over each book, which is a reference number\n",
    "\n",
    "    def get_gutenberg_text():\n",
    "        url = \"https://www.gutenberg.org/files/\" + str(counter) + \"/\" + str(counter) + \".txt\"\n",
    "\n",
    "        try: # Check if URL valid\n",
    "            webpage = urllib.request.urlopen(url) # Open the webpage containing book text\n",
    "\n",
    "            # Extract book title and author (author TBD) for file name\n",
    "            linecount = 1\n",
    "            for line in webpage:\n",
    "                m = re.search('Title: ',str(line))\n",
    "                if m:\n",
    "                    print(\"Matched!\")\n",
    "                    text = line.decode()\n",
    "                    booktitle = text[7 : (len(text) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                linecount += 1 # Advance line counter\n",
    "            filename = str(counter)+'_'+booktitle+'.txt'\n",
    "            \n",
    "            # Write book text to output file\n",
    "            print(\"Currently retrieving: \" + booktitle + \" -- file name: \" + filename)\n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Texts/' + filename),'w') as file:\n",
    "                webpage = urllib.request.urlopen(url)\n",
    "                for line in webpage:\n",
    "                    text = line.decode() # IMP: Extract only text, discarding non-printing characters\n",
    "                    file.write(text)\n",
    "            \n",
    "            # Write book text to list \n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Texts/' + filename),'r') as file:\n",
    "                text = [file.read().replace('\\n','')]\n",
    "                gutenberg_texts.append(text)\n",
    "                print(\"Added list item: \" + str(len(gutenberg_texts)) + \"\\n\") # Enumerate list count, which is number of books\n",
    "            return gutenberg_texts\n",
    "\n",
    "\n",
    "        except: # If URL invalid, means no book at that webpage\n",
    "            print(\"URL Not Valid\\n\")\n",
    "\n",
    "    gutenberg_texts = get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIK EXPLORATION: Separate function to populate list object using existing files in given folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Texts/') # Set working folder\n",
    "print(\"Adding *all* files in \" + os.getcwd() + \"\\n\")\n",
    "gutenberg_texts = [] # Initialize array list\n",
    "\n",
    "def get_gutenberg_text():\n",
    "    # Write book text to list \n",
    "    for file in sorted(os.listdir()):\n",
    "        with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = [f.read().replace('\\n','')]\n",
    "            # text = [f.read().replace('\\r','')]\n",
    "            gutenberg_texts.append(text)\n",
    "            print(str(len(gutenberg_texts)) + \": \" + file) # Enumerate list count, which is running count of books\n",
    "    return gutenberg_texts\n",
    "\n",
    "gutenberg_texts = get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gutenberg_texts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlist = ['one two three','four five six','seven eight nine','ten']\n",
    "print(testlist)\n",
    "testlist.append('eleven')\n",
    "print(testlist)\n",
    "testlist.insert(0,'Zero')\n",
    "print(testlist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **STOP** - Resume NLP lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# User function to scrape transcript data from scrapsfromtheloft.com\n",
    "def url_to_transcript(url):\n",
    "    '''Returns HTML contents of specified site.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find(class_=\"post-content\").find_all('p')]\n",
    "    print(url)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs of transcripts in scope\n",
    "urls = ['http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/07/bo-burnham-2013-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/10/21/hasan-minhaj-homecoming-king-2017-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/']\n",
    "\n",
    "# Comedian names\n",
    "comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actually perform scrape of contents of scrapsfromtheloft.com\n",
    "\n",
    "transcripts = [url_to_transcript(u) for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle files for later use - alternative to `csv_writer()`?\n",
    "\n",
    "## Make a new directory to hold the text files\n",
    "!mkdir transcripts\n",
    "\n",
    "for i, c in enumerate(comedians):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(transcripts[i], file) ### Indexing into the `transcripts` array/list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled files - equivalent to `read_csv`?\n",
    "data = {} # `{}` signifies a dictionary\n",
    "for i, c in enumerate(comedians):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"rb\") as file:\n",
    "        data[c] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check to make sure data has been loaded properly\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More checks\n",
    "data['louis'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our data again\n",
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: comedian, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['transcript']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the transcript for Ali Wong\n",
    "data_df.transcript.loc['ali']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}