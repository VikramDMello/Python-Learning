{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK EXPLORATION: Search Engine Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES & TIPS\n",
    "\n",
    "- Remember to set Python kernel to 3 (not later).\n",
    "- Import packages `os`, `requests`, `BeautifulSoup`, `csv`, `datetime`,es `textblob`, `wordcloud`, and `gensim`.\n",
    "- `get` works in conjunction with `requests`.\n",
    "- `BeautifulSoup` must have a particular HTML element from a webpage to work on.\n",
    "  E.g., , `class=\"post-content\">`, and the *p* is from `<p style=...>`.\n",
    "  + Each website might have its own HTML structure; so might need different `soup.find` argument for each site being scraped.  \n",
    "    * E.g., `class_=\"css-53u6y8\"` works for a NYTimes.com article, along with  *p* which is standard in HTML to represent a paragraph of text.\n",
    "    * E.g., `class_=\"repo-list\"` works for GitHub search results.\n",
    "    * E.g., `li class_=\"b_algo\"` with *a* works for Bing search result\n",
    "  + Can be tricky depending on webpage HTML structure; requires careful inspection even for different pages of same website, which might be slightly different.s    * What you see online is **not** an accurate representation of what `BeautifulSoup` sees; you must save the parsed HTML document to file and study that version..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages for scraping webpage contents and making sense of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "import csv \n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set search query values from lists - this needs to be formalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query_job = ['registered nurse','doctor','phone service rep'] # To be expanded from some source of top job titles\n",
    "search_query_location = ['new york city','dallas','topeka'] # To be expanded from some source of top metros\n",
    "\n",
    "### Instead, reference GSheet for each of the above items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set query URLs, conduct search, parse necessary information, and save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/Volumes/GoogleDrive/My Drive/Market Insights Library/00_Sources (needs organization)/Labor Market/Supply-Demand Dynamics/Job_Search_Results_Combined.csv','a') as csv_file: # Keep output file open at the beginning of all loops\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    for qj in search_query_job: # For each query's job keyword...\n",
    "        for ql in search_query_location: # ...and location keyword...\n",
    "            search_query = qj+\" \"+ql # ...form combined search query phrase\n",
    "            \n",
    "            # For Google:\n",
    "            query_url_google = 'https://www.google.com/search?q=' + str(qj) + \" jobs \" + str(ql) # ...which populates query URL for search engine\n",
    "            query_results_google = requests.get(query_url_google).text # Actually perform the search, resulting in unparsed HTML page object...\n",
    "            query_results_parsed_google = BeautifulSoup(query_results_google, 'lxml') # ...which is parsed by BeautifulSoup for structured searching\n",
    "\n",
    "            for result_google in query_results_parsed_google.find_all('div', attrs = {'class':'ZINbbc xpd O9g5cc uUPGi'}): # Iterate through each result...\n",
    "                try:\n",
    "                    result_google_title = result_google.find('div', attrs = {'class':'BNeawe vvjwJb AP7Wnd'}).text # ...looking for listing title...\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    result_google_source = result_google.find('div', attrs = {'class':'BNeawe UPmit AP7Wnd'}).text # ...listing source...\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    result_google_stub = result_google.find('div', attrs = {'class':'BNeawe s3v9rd AP7Wnd'}).text # ...and miscellaneous information below listing\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                csv_writer.writerow([datetime.today(),\"Google\",search_query,result_google_title,result_google_source,result_google_stub]) # Save this single result's values to file, before moving to the next result\n",
    "\n",
    "\n",
    "\n",
    "            # For Indeed:\n",
    "            query_url_indeed = 'https://www.indeed.com/jobs?q=' + str(qj) + \" jobs \" + str(ql)\n",
    "            query_results_indeed = requests.get(query_url_indeed).text\n",
    "            query_results_parsed_indeed = BeautifulSoup(query_results_indeed, 'lxml')\n",
    "\n",
    "            for result_indeed in query_results_parsed_indeed.find_all('div', attrs = {'class':'jobsearch-SerpJobCard unifiedRow row result'}):\n",
    "                try:\n",
    "                    result_indeed_title = result_indeed.find('h2', attrs = {'class':'title'}).text\n",
    "                    result_indeed_title = result_indeed_title.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    result_indeed_employer = result_indeed.find('span', attrs = {'class':'company'}).text\n",
    "                    result_indeed_employer =result_indeed_employer.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    result_indeed_location = result_indeed.find('span', attrs = {'class':'location accessible-contrast-color-location'}).text\n",
    "                    result_indeed_location = result_indeed_location.strip()\n",
    "                except:\n",
    "                    result_indeed_location=\"\"\n",
    "                try:\n",
    "                    result_indeed_summary = result_indeed.find('li').text\n",
    "                    result_indeed_summary = result_indeed_summary.strip()\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    result_indeed_postingdate = result_indeed.find('span', attrs = {'class':'date'}).text\n",
    "                    result_indeed_postingdate = result_indeed_postingdate.strip()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                csv_writer.writerow([datetime.today(),\"Indeed\",search_query,result_indeed_title,\"\",\"\",result_indeed_employer,result_indeed_location,result_indeed_summary,result_indeed_postingdate])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference template to extract result elements for first result check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test for Indeed\n",
    "\n",
    "query_url_indeed = 'https://www.indeed.com/jobs?q=nurse phlebotomist washington dc'\n",
    "query_results_indeed = requests.get(query_url_indeed).text\n",
    "query_results_parsed_indeed = BeautifulSoup(query_results_indeed, 'lxml')\n",
    "\n",
    "for result_indeed in query_results_parsed_indeed.find_all('div', attrs = {'class':'jobsearch-SerpJobCard unifiedRow row result'}):\n",
    "    try:\n",
    "        result_indeed_title = result_indeed.find('h2', attrs = {'class':'title'}).text\n",
    "        result_indeed_title = result_indeed_title.strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        result_indeed_employer = result_indeed.find('span', attrs = {'class':'company'}).text\n",
    "        result_indeed_employer =result_indeed_employer.strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        result_indeed_location = result_indeed.find('span', attrs = {'class':'location accessible-contrast-color-location'}).text\n",
    "        result_indeed_location = result_indeed_location.strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        result_indeed_summary = result_indeed.find('li').text\n",
    "        result_indeed_summary = result_indeed_summary.strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        result_indeed_postingdate = result_indeed.find('span', attrs = {'class':'date'}).text\n",
    "        result_indeed_postingdate = result_indeed_postingdate.strip()\n",
    "    except:\n",
    "        pass\n",
    "  \n",
    "\n",
    "\n",
    "### Print first result's extracted elements as check\n",
    "print(\"First Indeed result as check:\\n\")\n",
    "print(\"\\tResult title: \" + result_indeed_title)\n",
    "print(\"\\tEmployer: \" + result_indeed_employer)\n",
    "print(\"\\tLocation: \" + result_indeed_location)\n",
    "print(\"\\tSummary: \" + result_indeed_summary)\n",
    "print(\"\\tPosting Date: \" + result_indeed_postingdate)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reference template to extract result elements and save to ongoing CSV file by appending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import csv # CSV module required for writing to CSV file\n",
    "from datetime import datetime\n",
    "\n",
    "#### Note `'a'` argument for CSV file append\n",
    "#### Since using csv.writer in append mode, no need for header row; but otherwise for new files must use:\n",
    "#### csv_writer.writerow(['Search_Date_Time','Search_Engine','Search_Query','Result_Title','Result_Source','Result_Stub'])\n",
    "\n",
    "with open('/Users/vix/Repos/Python-Learning/src/NLP/Labor Market/Search_Results_Combined.csv','a') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    for result_google in query_results_parsed_google.find_all('div', attrs = {'class':'ZINbbc xpd O9g5cc uUPGi'}):\n",
    "        result_google_title = result_google.find('div', attrs = {'class':'BNeawe vvjwJb AP7Wnd'}).text\n",
    "        result_google_source = result_google.find('div', attrs = {'class':'BNeawe UPmit AP7Wnd'}).text\n",
    "        result_google_stub = result_google.find('div', attrs = {'class':'BNeawe s3v9rd AP7Wnd'}).text\n",
    "        csv_writer.writerow([datetime.today(),\"Google\",search_query,result_google_title,result_google_source,result_google_stub])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot-checks and future expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result_google_employer = result_google.find_next_sibling('div', attrs = {'class':'vNEEBe'}).text\n",
    "    result_google_location = result_google.find('div', attrs = {'class':'Qk80Jf'}).text\n",
    "    result_google_postingdate = result_google.find('span', attrs = {'class':'SuWscb'}).text\n",
    "    \n",
    "query_url_bing = 'https://www.bing.com/search?q=' + str(search_query)\n",
    "query_results_bing = requests.get(query_url_bing).text\n",
    "query_results_parsed_bing = BeautifulSoup(query_results_bing, 'html.parser')\n",
    "\n",
    "for result_bing in query_results_parsed_bing.find_all('ul', attrs = {'class':'b_hList'}):\n",
    "    result_bing_title = results_bing.find('div', attrs = {'class':'jb_title'}).text\n",
    "    result_bing_employer = results_bing.find('div', attrs = {'class':'jb_company'}).text\n",
    "    result_bing_location = results_bing.find('div', attrs = {'class':'jb_loc_jobType'}).text\n",
    "    result_bing_salary = results_bing.find('div', attrs = {'class':'jb_salary'}).text\n",
    "\n",
    "#### `BeautifulSoup` metadata\n",
    "tag = query_html_bing.div\n",
    "type(tag)\n",
    "print(\"\\n\" + str(tag) + \"\\n\")\n",
    "print(query_html_bing.div.get_attribute_list('class'))\n",
    "\n",
    "#### Save parsed HTML page as file for review & testing\n",
    "parsed_html_text = str(query_results_parsed_google) \n",
    "parsed_html_file = open(\"/Users/vix/Repos/Python-Learning/src/NLP/Labor Market/parsed_html.html\", 'w')\n",
    "parsed_html_file.write(parsed_html_text)\n",
    "parsed_html_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask user for search query input - Not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "os.system('clear')\n",
    "\n",
    "print(\"\\n\\nHello there!  \\n\\n\\nThis tool takes your search query, \\n\\napplies it to both major general-purpose search engines (Google & Bing), \\n\\nand to one major job search engine (Indeed), \\n\\nand then displays a simple comparison of the resulting search engine results.  \\n\\nAnalysis is limited to the first 100 results from each search engine.\")\n",
    "\n",
    "search_query = input(\"\\n\\n\\nWhat should we search for? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK EXPLORATION: Compare two text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import urllib\n",
    "import os\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Texts')\n",
    "\n",
    "with open('base.txt', 'r') as file1:\n",
    "    with open('comparison.txt', 'r') as file2:\n",
    "        difference = set(file1).symmetric_difference(file2)\n",
    "\n",
    "for line in difference:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK EXPLORATION: Retrieve contents of text **file** at any URL and save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK EXPLORATION: File operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Essentially copying a file's contents to another file - this works for text files...\n",
    "\n",
    "with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/README.txt','r') as sourcefile:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/DESTFILE.txt','w') as destfile:\n",
    "        for line in sourcefile:\n",
    "            destfile.write(line)\n",
    "\n",
    "\n",
    "### And for an image file...simply append `b` for *binary mode* to file operation command `r`, `w`, or `a`\n",
    "\n",
    "with open('/Users/vix/OneDrive/Temp/Portrait_Vikram_Before-After.png','rb') as sourceimage:\n",
    "    with open('/Users/vix/Repos/Python-Learning/src/NLP/SNLI Stanford Corpus/destimage.png','wb') as destimage:\n",
    "        for line in sourceimage:\n",
    "            destimage.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK EXPLORATION: Get book text from Project Gutenberg, save to file, and populate list object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "import re\n",
    "import urllib # Import `urllib` package - primarily using `request` module with `urlopen` method\n",
    "\n",
    "# os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Texts')\n",
    "gutenberg_texts = [] # Initialize list\n",
    "\n",
    "for counter in range(10,25003): # Loop over each book, which is a reference number\n",
    "\n",
    "    def get_gutenberg_text():\n",
    "        url = \"https://www.gutenberg.org/files/\" + str(counter) + \"/\" + str(counter) + \".txt\"\n",
    "\n",
    "        try: # Check if URL valid\n",
    "            webpage = urllib.request.urlopen(url) # Open the webpage containing book text\n",
    "\n",
    "            # Extract book title and author (author TBD) for file name\n",
    "            linecount = 1\n",
    "            for line in webpage:\n",
    "                m = re.search('Title: ',str(line))\n",
    "                if m:\n",
    "                    print(\"Matched!\")\n",
    "                    text = line.decode()\n",
    "                    booktitle = text[7 : (len(text) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                linecount += 1 # Advance line counter\n",
    "            filename = str(counter)+'_'+booktitle+'.txt'\n",
    "            \n",
    "            # Write book text to output file\n",
    "            print(\"Currently retrieving: \" + booktitle + \" -- file name: \" + filename)\n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Texts/' + filename),'w') as file:\n",
    "                webpage = urllib.request.urlopen(url)\n",
    "                for line in webpage:\n",
    "                    text = line.decode() # IMP: Extract only text, discarding non-printing characters\n",
    "                    file.write(text)\n",
    "            \n",
    "            # Write book text to list \n",
    "            with open(str('/Users/vix/Repos/Python-Learning/src/NLP/Texts/' + filename),'r') as file:\n",
    "                text = [file.read().replace('\\n','')]\n",
    "                gutenberg_texts.append(text)\n",
    "                print(\"Added list item: \" + str(len(gutenberg_texts)) + \"\\n\") # Enumerate list count, which is number of books\n",
    "            return gutenberg_texts\n",
    "\n",
    "\n",
    "        except: # If URL invalid, means no book at that webpage\n",
    "            print(\"URL Not Valid\\n\")\n",
    "\n",
    "    gutenberg_texts = get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIK EXPLORATION: Separate function to populate list object using existing files in given folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n",
    "\n",
    "os.chdir('/Users/vix/Repos/Python-Learning/src/NLP/Intro to NLP Alice Zhao/books') # Set working folder\n",
    "print(\"Adding *all* files in \" + os.getcwd() + \"\\n\")\n",
    "gutenberg_texts = [] # Initialize list of texts\n",
    "gutenberg_titles = [] # Initialize list of book titles\n",
    "\n",
    "def get_gutenberg_text():\n",
    "    # Extract book title\n",
    "    for file in sorted(listdir_nohidden(\".\")):\n",
    "        with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            linecount = 1\n",
    "            for line in f:\n",
    "                m = re.search('Title: ',str(line))\n",
    "                if m:\n",
    "                    print(\"Matched!\")\n",
    "                    title = line[7 : (len(line) - 2)] # Minus 2 at end critical to remove newline character\n",
    "                    gutenberg_titles.append(title) # Put book titles in list\n",
    "                linecount += 1 # Advance line counter\n",
    "\n",
    "    # Write book text from file to list \n",
    "    for file in sorted(listdir_nohidden(\".\")):\n",
    "        with codecs.open(file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            text = [f.read().replace('\\n','')]\n",
    "            gutenberg_texts.append(text)\n",
    "            print(str(len(gutenberg_texts)) + \": \" + file) # Enumerate list count, which is running count of books\n",
    "\n",
    "get_gutenberg_text() # Call function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Useful code to display beginnings of each list item as preview\n",
    "[book[0][:100] for book in gutenberg_texts]\n",
    "\n",
    "### Similar code for items in dictionary form\n",
    "{}\n",
    "\n",
    "### Useful code to convert MS Word document to text file\n",
    "\n",
    "import docx2txt\n",
    "converted_text = docx2txt.process(filename.docx)\n",
    "with open(Filename.txt, 'w') as file:\n",
    "    file.write(converted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom function to create `listdir` command that does not show hidden files\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    import glob\n",
    "    return glob.glob(os.path.join(path, '*'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_titles.append(\"Q and the Magic of Grammar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STOP** - Resume NLP lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# User function to scrape transcript data from scrapsfromtheloft.com\n",
    "def url_to_transcript(url):\n",
    "    '''Returns HTML contents of specified site.'''\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = [p.text for p in soup.find(class_=\"post-content\").find_all('p')]\n",
    "    print(url)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLs of transcripts in scope\n",
    "urls = ['http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/07/bo-burnham-2013-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/10/21/hasan-minhaj-homecoming-king-2017-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
    "        'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/']\n",
    "\n",
    "# Comedian names\n",
    "comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Actually perform scrape of contents of scrapsfromtheloft.com\n",
    "\n",
    "transcripts = [url_to_transcript(u) for u in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle files for later use - alternative to `csv_writer()`?\n",
    "\n",
    "## Make a new directory to hold the text files\n",
    "!mkdir transcripts\n",
    "\n",
    "for i, c in enumerate(comedians):\n",
    "    with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
    "        pickle.dump(transcripts[i], file) ### Indexing into the `transcripts` array/list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickled files - modified code to simply open file; unsure about need for Pickle\n",
    "### Create dictionary data container which can hold book text as well as title\n",
    "\n",
    "books = {} # `{}` signifies a dictionary\n",
    "for i, title in enumerate(gutenberg_titles):\n",
    "    with open(title + \".txt\", 'r') as f:\n",
    "        books[title] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "source": [
    "books = {'title': gutenberg_titles, 'text': gutenberg_texts}\n",
    "print(len(books))\n",
    "for key, value in books.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double check to make sure data has been loaded properly\n",
    "books.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More checks\n",
    "books['The King James Bible'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at our data again\n",
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are going to change this to key: comedian, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth',150)\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['transcript']\n",
    "data_df = data_df.sort_index()\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's take a look at the transcript for Ali Wong\n",
    "data_df.transcript.loc['ali']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}